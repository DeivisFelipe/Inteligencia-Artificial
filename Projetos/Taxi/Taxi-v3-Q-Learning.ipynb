{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Importando o Gym\n",
    "import gym\n",
    "# Criando \n",
    "env = gym.make(\"Taxi-v3\", render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(286,\n",
       " -1,\n",
       " False,\n",
       " False,\n",
       " {'prob': 1.0, 'action_mask': array([1, 1, 0, 1, 0, 0], dtype=int8)})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "obs, info = env.reset(seed=42)\n",
    "obs\n",
    "\n",
    "env.action_space\n",
    "\n",
    "obs = env.step(1)\n",
    "obs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metodos que sera utilizado\n",
    "method = 1\n",
    "\n",
    "if method == 1:\n",
    "\n",
    "    env.reset()\n",
    "    # Listando as ações\n",
    "    print(\"Total de Ações {}\".format(env.action_space))\n",
    "    # Total de Estados\n",
    "    print(\"Total de Estados {}\".format(env.observation_space))\n",
    "    # Troca de Estado\n",
    "    state = env.encode(3, 1, 2, 1)\n",
    "    env.s = state\n",
    "    env.render()\n",
    "\n",
    "    # Rewards\n",
    "    # -1 por cada passo ou bater em uma parede\n",
    "    # -10 pegar ou deixar um passageiro em um local errado\n",
    "    # 20 pegar ou deixar um passageiro em um local correto\n",
    "\n",
    "    # Tabela de recompensas, onde cada linha é um estado e cada coluna uma ação, total de 500 estados e 6 ações possíveis = 3000\n",
    "    env.P[329]\n",
    "\n",
    "    # Tentando resolver o problema com ações aleatórias\n",
    "    env.s = 329\n",
    "    env.reset()\n",
    "    epochs = 0 # Contador de passos\n",
    "    penalties = 0 # Contador de penalidades\n",
    "    done = False # Flag para indicar se o jogo terminou\n",
    "    truncated = False # Flag para indicar se o jogo foi truncado\n",
    "\n",
    "    while not done and not truncated:\n",
    "        action = env.action_space.sample() # Escolhe uma ação aleatória\n",
    "        state, reward, done, truncated, info = env.step(action) # Executa a ação\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "        \n",
    "        env.render()\n",
    "\n",
    "        epochs += 1\n",
    "\n",
    "    print(\"Timesteps tomados: {}\".format(epochs)) # Total de passos\n",
    "    print(\"Penalidades recebidas: {}\".format(penalties)) # Total de penalidades\n",
    "\n",
    "    # Finaliza o ambiente\n",
    "    env.close()\n",
    "elif method == 2:\n",
    "    import numpy as np\n",
    "    import time\n",
    "\n",
    "    # Inicializa a tabela de valores Q\n",
    "    # verifica se existe uma tabela Q salva no computador\n",
    "    try:\n",
    "        q_table = np.load(\"q_table.npy\")\n",
    "        print(\"Carregando tabela Q\")\n",
    "    except:\n",
    "        print(\"Criando tabela Q\")\n",
    "        q_table = np.zeros(\n",
    "            [env.observation_space.n, env.action_space.n]\n",
    "        )\n",
    "\n",
    "    import random\n",
    "\n",
    "    # Hiperparâmetros\n",
    "    alpha = 0.1 # Taxa de aprendizado\n",
    "    gamma = 0.6 # Fator de desconto\n",
    "    epsion = 0.1 # Taxa de exploração\n",
    "\n",
    "    # Total de ações e penalidades recebidas durando a aprendizagem\n",
    "    epochs = 0\n",
    "    penalties = 0\n",
    "\n",
    "    for i in range(1, 1): # 100000 versoões de treinamento\n",
    "        state = env.reset() # incialização aleatória do ambiente\n",
    "        state = state[0]\n",
    "        done = False\n",
    "        truncated = False\n",
    "        rewards = 0\n",
    "        epsodies = 0\n",
    "\n",
    "        # Salva o q-table de forma definitiva no computador\n",
    "        if i % 100000 == 0:\n",
    "            np.save(\"q_table.npy\", q_table)\n",
    "            print(\"Salvando tabela Q\")\n",
    "            time.sleep(1)\n",
    "\n",
    "        while not done and not truncated and epsodies < 200:\n",
    "            if random.uniform(0, 1) < epsion:\n",
    "                action = env.action_space.sample() # Escolhe uma ação aleatória\n",
    "            else:\n",
    "                action = np.argmax(q_table[state]) # Escolhe a melhor ação\n",
    "\n",
    "            next_state, reward, done, truncated, info = env.step(action) # Executa a ação\n",
    "\n",
    "            # Somatório das recompensas\n",
    "            rewards += reward\n",
    "\n",
    "            old_value = q_table[state, action] # Valor antigo da tabela Q\n",
    "            next_max = np.max(q_table[next_state]) # Valor máximo da tabela Q para o próximo estado\n",
    "\n",
    "            # Atualiza a tabela Q com base na equação de Bellman\n",
    "            new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "            q_table[state, action] = new_value\n",
    "\n",
    "            if reward == -10:\n",
    "                penalties += 1\n",
    "\n",
    "            state = next_state # Mudança de estado\n",
    "            epsodies += 1\n",
    "\n",
    "        print(\"Geração: {}, Episódio: {}, Recompensas: {}\".format(i, epsodies, rewards))\n",
    "\n",
    "\n",
    "    print(\"Treinamento finalizado.\\n\")\n",
    "\n",
    "    #printa as 5 primeiras linhas da tabela Q\n",
    "    print(q_table[1:2])\n",
    "\n",
    "    # Executa o ambiente com a tabela Q treinada, 100 vezes\n",
    "    epochs = 0\n",
    "    penalties = 0\n",
    "    env = gym.make(\"Taxi-v3\", render_mode=\"human\")\n",
    "    \n",
    "    for i in range(1, 101):\n",
    "        state = env.reset()\n",
    "        state = state[0]\n",
    "        done = False\n",
    "        truncated = False\n",
    "        rewards = 0\n",
    "        epsodies = 0\n",
    "\n",
    "        while not done and not truncated and epsodies < 200:\n",
    "            action = np.argmax(q_table[state])\n",
    "            state, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "            rewards += reward\n",
    "\n",
    "            if reward == -10:\n",
    "                penalties += 1\n",
    "\n",
    "            epsodies += 1\n",
    "\n",
    "        print(\"Geração: {}, Episódio: {}, Recompensas: {}, Penalidades: {}\".format(i, epsodies, rewards, penalties))\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
